{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "P1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIjVAMs5m5hd"
      },
      "source": [
        "# P1: Solve the OpenAI Gym [Taxi V3](https://gym.openai.com/envs/Taxi-v3/) Environment\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-rgpgt-m5hi"
      },
      "source": [
        "## Introduction\n",
        "[OpenAI Gym](https://gym.openai.com/docs/) is a framework that provides RL environments of varying complexity with the same standard API making it easy to develop and benchmark RL algorithms. The [Taxi-V3](https://gym.openai.com/envs/Taxi-v3/) environmnet present a simple, text environment where actions and state (observations) are both discrete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev1r_Rn4m5hk"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X4OFw_am5hl"
      },
      "source": [
        "The `gym.make()` API can be used to spawn any of the available environments by passing its full name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIHdIs5Qm5hm"
      },
      "source": [
        "taxi = gym.make('Taxi-v3')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuVuIRMlm5ho"
      },
      "source": [
        "The Taxi environment has 500 states and 6 possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcFcihpsm5hp",
        "outputId": "267433f2-5afa-4a91-f7e8-3e0e223edf38"
      },
      "source": [
        "taxi.action_space"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBbeLp0Qm5hr",
        "outputId": "35352247-09e8-407d-9871-1bb721ab609b"
      },
      "source": [
        "taxi.observation_space"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF0-O5Jhm5hs"
      },
      "source": [
        "The task and reward structure are described in the [documentation](https://github.com/openai/gym/blob/a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022/gym/envs/toy_text/taxi.py#L25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6n0OkpPm5ht",
        "outputId": "3d60588c-28a0-4081-fca3-9dbc5962da33"
      },
      "source": [
        "taxi.reset()\n",
        "taxi.render()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Gsn33wm5hu"
      },
      "source": [
        "## Submission\n",
        "- Submit your solution as a Jupyter notebook. \n",
        "- Ensure that all cells in the notebook have been executed and the output is showing\n",
        "- Ensure that your solution consistently reaches the average cumulative reward defined in the rubric (link below)\n",
        "- Post your solution on Github and share the link to your commit as a direct message in Slack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP6Y7lZ5jTiZ"
      },
      "source": [
        "**1**\n",
        "Describe the methods and variables in the class DiscreteEnv which is the parent class of the Taxi V3 class.\n",
        "\n",
        "**Variables:**\n",
        "\n",
        "nS: number of states\n",
        "\n",
        "nA: number of actions\n",
        "\n",
        "action_space: The Space object corresponding to valid actions\n",
        "\n",
        "observation_space: The Space object corresponding to valid observations\n",
        "\n",
        "s: current state\n",
        "\n",
        "lastaction: action taken in the last step\n",
        "\n",
        "P: A dictionary defining transitions. For each key (state s and action a), the value is a list of tuples. Each tuple defines the probability of reaching a possible next state (from given state and action), the specified next state, the associated reward and whether the task is done: \n",
        "P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "\n",
        "isd: An array of length nS defining the initial state distribution \n",
        "\n",
        "**Methods:**\n",
        "\n",
        "step(self, action): \n",
        "\n",
        "*   Run one timestep of the environment's dynamics.\n",
        "*   Returns:\n",
        "            observation (object): agent's observation of the current\n",
        "            environment\n",
        "            reward (float) : amount of reward returned after previous action\n",
        "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
        "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
        "\n",
        "\n",
        "reset(self):\n",
        "\n",
        "\n",
        "*   Run one timestep of the environment's dynamics.\n",
        "*   List item\n",
        "\n",
        "\n",
        "*   Returns:\n",
        "            observation (object): the initial observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x27p79iYcuKH"
      },
      "source": [
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "    \n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns,p=action_probs)\n",
        "    \n",
        "    return epsilon_greedy_action"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDKprtbWczh_",
        "outputId": "0a63e2d2-c3e8-436a-dfb9-24a7c2c086b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#from tqdm import tqdm \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "Q = pd.DataFrame.from_dict({s:{a:0 for a in range(taxi.nA)} for s in range(taxi.nS)}, orient='index')\n",
        "\n",
        "HYPER_PARAMS = {'gamma':0.9}\n",
        "\n",
        "n_episodes = 5000\n",
        "max_episode_len = 100\n",
        "epsilon = 1\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay = 0.99\n",
        "alpha = 0.1\n",
        "\n",
        "rewards = np.zeros(n_episodes)\n",
        "\n",
        "for i in range(n_episodes):  \n",
        "    taxi.reset()\n",
        "    s0 = taxi.s\n",
        "    done = False\n",
        "    \n",
        "    episode_reward = 0\n",
        "\n",
        "    for step in range(max_episode_len):\n",
        "        a0 = epsilon_greedy_action_from_Q(Q,s0,epsilon)\n",
        "        out  = taxi.step(a0)\n",
        "        s1 = out[0]\n",
        "        reward = out[1]\n",
        "        done = out[2]\n",
        "        \n",
        "        Q.loc[s0,a0] += alpha*(reward + HYPER_PARAMS['gamma']*Q.loc[s1].max() - Q.loc[s0,a0])\n",
        "        episode_reward += reward\n",
        "        s0 = s1\n",
        "\n",
        "        assert (reward!=-10) or (i<n_episodes-1000)\n",
        "\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "    if i%100 == 0:\n",
        "      print (\"reward for episode {}: {}\".format(i, episode_reward))\n",
        "  \n",
        "    epsilon *= epsilon_decay\n",
        "    epsilon = max(epsilon,min_epsilon) if i<n_episodes-1000 else 0  \n",
        "\n",
        "    rewards[i] = episode_reward\n",
        "        "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward for episode 0: -397\n",
            "reward for episode 100: -271\n",
            "reward for episode 200: -118\n",
            "reward for episode 300: -109\n",
            "reward for episode 400: -100\n",
            "reward for episode 500: -74\n",
            "reward for episode 600: -2\n",
            "reward for episode 700: -95\n",
            "reward for episode 800: -73\n",
            "reward for episode 900: -73\n",
            "reward for episode 1000: 14\n",
            "reward for episode 1100: -11\n",
            "reward for episode 1200: -25\n",
            "reward for episode 1300: 8\n",
            "reward for episode 1400: 10\n",
            "reward for episode 1500: -5\n",
            "reward for episode 1600: 9\n",
            "reward for episode 1700: 8\n",
            "reward for episode 1800: -3\n",
            "reward for episode 1900: 14\n",
            "reward for episode 2000: 5\n",
            "reward for episode 2100: -2\n",
            "reward for episode 2200: 8\n",
            "reward for episode 2300: 7\n",
            "reward for episode 2400: 9\n",
            "reward for episode 2500: 3\n",
            "reward for episode 2600: 6\n",
            "reward for episode 2700: 7\n",
            "reward for episode 2800: 8\n",
            "reward for episode 2900: 8\n",
            "reward for episode 3000: 10\n",
            "reward for episode 3100: 9\n",
            "reward for episode 3200: 13\n",
            "reward for episode 3300: 5\n",
            "reward for episode 3400: 10\n",
            "reward for episode 3500: 8\n",
            "reward for episode 3600: 5\n",
            "reward for episode 3700: 9\n",
            "reward for episode 3800: 12\n",
            "reward for episode 3900: 9\n",
            "reward for episode 4000: 10\n",
            "reward for episode 4100: 15\n",
            "reward for episode 4200: 4\n",
            "reward for episode 4300: 7\n",
            "reward for episode 4400: 10\n",
            "reward for episode 4500: 8\n",
            "reward for episode 4600: 8\n",
            "reward for episode 4700: 6\n",
            "reward for episode 4800: 10\n",
            "reward for episode 4900: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06ng5Utc8Gw",
        "outputId": "f3589447-1013-4642-d541-c4804bd50f59"
      },
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "windowed_rewards = np.convolve(rewards[-1000:], np.ones(100), 'valid')\n",
        "np.quantile(windowed_rewards/100,[0.05, 0.95])\n",
        "#plt.plot(windowed_rewards/100)\n",
        "#plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.41, 8.53])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63nRou-m5hv"
      },
      "source": [
        "## Evaluation\n",
        "The goal of the project is to get a certain average (cumulative) reward over 100 episodes. To pass the project, you must meet all the requirments in the project [rubric](https://github.com/KnowchowHQ/rl-in-action/blob/master/C1-RL-Intro/W3OH/P1-rubric.md)"
      ]
    }
  ]
}