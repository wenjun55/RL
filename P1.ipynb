{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "P1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIjVAMs5m5hd"
      },
      "source": [
        "# P1: Solve the OpenAI Gym [Taxi V3](https://gym.openai.com/envs/Taxi-v3/) Environment\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-rgpgt-m5hi"
      },
      "source": [
        "## Introduction\n",
        "[OpenAI Gym](https://gym.openai.com/docs/) is a framework that provides RL environments of varying complexity with the same standard API making it easy to develop and benchmark RL algorithms. The [Taxi-V3](https://gym.openai.com/envs/Taxi-v3/) environmnet present a simple, text environment where actions and state (observations) are both discrete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev1r_Rn4m5hk"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X4OFw_am5hl"
      },
      "source": [
        "The `gym.make()` API can be used to spawn any of the available environments by passing its full name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIHdIs5Qm5hm"
      },
      "source": [
        "taxi = gym.make('Taxi-v3')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuVuIRMlm5ho"
      },
      "source": [
        "The Taxi environment has 500 states and 6 possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcFcihpsm5hp",
        "outputId": "267433f2-5afa-4a91-f7e8-3e0e223edf38"
      },
      "source": [
        "taxi.action_space"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBbeLp0Qm5hr",
        "outputId": "35352247-09e8-407d-9871-1bb721ab609b"
      },
      "source": [
        "taxi.observation_space"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF0-O5Jhm5hs"
      },
      "source": [
        "The task and reward structure are described in the [documentation](https://github.com/openai/gym/blob/a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022/gym/envs/toy_text/taxi.py#L25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6n0OkpPm5ht",
        "outputId": "3d60588c-28a0-4081-fca3-9dbc5962da33"
      },
      "source": [
        "taxi.reset()\n",
        "taxi.render()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Gsn33wm5hu"
      },
      "source": [
        "## Submission\n",
        "- Submit your solution as a Jupyter notebook. \n",
        "- Ensure that all cells in the notebook have been executed and the output is showing\n",
        "- Ensure that your solution consistently reaches the average cumulative reward defined in the rubric (link below)\n",
        "- Post your solution on Github and share the link to your commit as a direct message in Slack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP6Y7lZ5jTiZ"
      },
      "source": [
        "# **1**\n",
        "**Describe the methods and variables in the class DiscreteEnv which is the parent class of the Taxi V3 class.**\n",
        "\n",
        "**Variables:**\n",
        "\n",
        "nS: number of states\n",
        "\n",
        "nA: number of actions\n",
        "\n",
        "action_space: The Space object corresponding to valid actions\n",
        "\n",
        "observation_space: The Space object corresponding to valid observations\n",
        "\n",
        "s: current state (observation)\n",
        "\n",
        "lastaction: action taken in the last step\n",
        "\n",
        "P: A dictionary defining transitions. For each key (state s and action a), the value is a list of tuples. Each tuple defines the probability of reaching a possible next state (from given state and action), the specified next state, the associated reward and whether the episode is done: \n",
        "P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "\n",
        "isd: An array of length nS defining the initial state distribution \n",
        "\n",
        "**Methods:**\n",
        "\n",
        "__init__(self, nS, nA, P, isd):\n",
        "\n",
        ">   Initialize the DiscreteEnv object with the given input information (number of states, number of actions, transitions and initial state distribution), and draws a random initial state. \n",
        "\n",
        "seed(self, seed):\n",
        "\n",
        ">   Sets the seed for this env's random number generator(s).\n",
        "\n",
        ">   Returns:\n",
        "*   the list of seeds used in this env's random number generators.\n",
        "\n",
        "step(self, action): \n",
        "\n",
        "> Run one timestep of the discrete environment's dynamics according to the transition table P.\n",
        "\n",
        "> Returns:\n",
        "*   observation (object): agent's observation of the current environment, i.e., the new state\n",
        "*   reward (float) : amount of reward returned after previous action\n",
        "*   done (bool): whether the episode has ended\n",
        "*   info (dict): contains auxiliary information, in this case the probability p associated with this new state\n",
        "\n",
        "reset(self):\n",
        "\n",
        ">   Run one timestep of the environment's dynamics.\n",
        "\n",
        ">   Returns:\n",
        "*   observation (object): the initial observation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFwJUC9sXsPW"
      },
      "source": [
        "# **2**\n",
        "**Describe the methods and variables in the Taxi V3 class.**\n",
        "\n",
        "**Variables:**\n",
        "\n",
        "desc: 2-dimensional array describing the map of the taxi environment.\n",
        "\n",
        "locs: A list of valid passenger pick-up and drop-off locations in terms of row and column indices in the map. \n",
        "\n",
        "**Methods**\n",
        "\n",
        "__init__(self):\n",
        "\n",
        "Initialize the Taxi V3 object, including its class variables. The dictionary P defining the transitions (probabilities, next states, rewards, done) from all states and all different actions are defined. The initial state distribution is computed. The init function of the parent class is called to initialize the class variables in DiscreteEnv.\n",
        "\n",
        "encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
        "\n",
        "> Encode taxi locations （taxi_row, taxi_col）, passenger location (pass_loc), and destination index (dest_idx) into the corresponding state.\n",
        "\n",
        "> Returns: \n",
        "* State \n",
        "\n",
        "decode(self, i):\n",
        "\n",
        "> Decode the taxi locations, passenger location and destination location from the corresponding state.\n",
        "\n",
        "> Returns:\n",
        "* Taxi location (taxi_row, taxi_col), passenger location(pass_idx), and destination index (dest_idx)\n",
        "\n",
        "render(self, mode):\n",
        "\n",
        ">   Renders the environment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK4QirQohlke"
      },
      "source": [
        "# **3** \n",
        "**Describe the Taxi V3 environment, its actions, states, reward structure and the rationale behind such a reward structure.**\n",
        "\n",
        "Taxi V3 environment has 500 discrete states, corresponding to 25 taxi locations on the 5x5 map, 5 passenger locations (4 possible pickup locations identified as R, G, B and Y on the map, plus passenger in the taxi), and 4 destination locations (R, G, B and Y). \n",
        "\n",
        "At each step the taxi can take one of 6 different actions: 4 actions corresponding to moving 1 unit in east, west, north, south directions, pick up passenger, and drop off passenger.\n",
        "\n",
        "The reward structure is as follows: \n",
        "* Delivering the passenger at proper destination: +20,\n",
        "* Executing \"pickup\" and \"drop-off\" actions illegally: -10,\n",
        "* All other cases: -1.\n",
        "\n",
        "Since the goal is to deliver the passenger at the right destination, it makes sense to have a large reward (20) when this task is accomplished.\n",
        "\n",
        "Meanwhile, it is desirable to take as fewer steps as possible to deliver the passenger, so for each additional step taken to accomplish the task, a -1 reward is incurred. \n",
        "\n",
        "The illegal pickup happens if the passenger is not at the taxi location at the action of pickup. In this case, the pickup action is wasted and implies that the taxi will take more time to reach the passenger location. The -10 reward is for the algorithm to learn to move to passenger location and only pick up at the passenger location. \n",
        "\n",
        "The illegal drop-off happens if the passenger is not in the car, or the taxi location is not at one of the 4 possible destination points at the action of drop-off. The dropoff action is wasted in this case, and the taxi will take more time to pick up the passenger (if needed) and go the desired destination. The -10 reward is for the algorithm to learn that dropoff should only happens when the passenger is already picked up and the taxi has moved to one of the possible destinations. Note that dropping-off at possible destination points other than the desired one only incurs -1 reward, possibly because it is legal to drop off there and the taxi can make up by picking up the passenger again at this point in the next step. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMHYQpFhtmU"
      },
      "source": [
        "# **4**\n",
        "**Train an algorithm to achieve a 100-episode average reward with a 5th percentile of 7.2 or higher and a 95th percentile of 8.2 or higher on the last 1000 episodes.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x27p79iYcuKH"
      },
      "source": [
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "    \n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns,p=action_probs)\n",
        "    \n",
        "    return epsilon_greedy_action"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDKprtbWczh_",
        "outputId": "0a63e2d2-c3e8-436a-dfb9-24a7c2c086b0"
      },
      "source": [
        "#from tqdm import tqdm \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "Q = pd.DataFrame.from_dict({s:{a:0 for a in range(taxi.nA)} for s in range(taxi.nS)}, orient='index')\n",
        "\n",
        "HYPER_PARAMS = {'gamma':0.9}\n",
        "\n",
        "n_episodes = 5000\n",
        "max_episode_len = 100\n",
        "epsilon = 1\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay = 0.99\n",
        "alpha = 0.1\n",
        "\n",
        "rewards = np.zeros(n_episodes)\n",
        "\n",
        "for i in range(n_episodes):  \n",
        "    taxi.reset()\n",
        "    s0 = taxi.s\n",
        "    done = False\n",
        "    \n",
        "    episode_reward = 0\n",
        "\n",
        "    for step in range(max_episode_len):\n",
        "        a0 = epsilon_greedy_action_from_Q(Q,s0,epsilon)\n",
        "        out  = taxi.step(a0)\n",
        "        s1 = out[0]\n",
        "        reward = out[1]\n",
        "        done = out[2]\n",
        "        \n",
        "        Q.loc[s0,a0] += alpha*(reward + HYPER_PARAMS['gamma']*Q.loc[s1].max() - Q.loc[s0,a0])\n",
        "        episode_reward += reward\n",
        "        s0 = s1\n",
        "\n",
        "        assert (reward!=-10) or (i<n_episodes-1000)\n",
        "\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "    if i%100 == 0:\n",
        "      print (\"reward for episode {}: {}\".format(i, episode_reward))\n",
        "  \n",
        "    epsilon *= epsilon_decay\n",
        "    epsilon = max(epsilon,min_epsilon) if i<n_episodes-1000 else 0  \n",
        "\n",
        "    rewards[i] = episode_reward\n",
        "        "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward for episode 0: -397\n",
            "reward for episode 100: -271\n",
            "reward for episode 200: -118\n",
            "reward for episode 300: -109\n",
            "reward for episode 400: -100\n",
            "reward for episode 500: -74\n",
            "reward for episode 600: -2\n",
            "reward for episode 700: -95\n",
            "reward for episode 800: -73\n",
            "reward for episode 900: -73\n",
            "reward for episode 1000: 14\n",
            "reward for episode 1100: -11\n",
            "reward for episode 1200: -25\n",
            "reward for episode 1300: 8\n",
            "reward for episode 1400: 10\n",
            "reward for episode 1500: -5\n",
            "reward for episode 1600: 9\n",
            "reward for episode 1700: 8\n",
            "reward for episode 1800: -3\n",
            "reward for episode 1900: 14\n",
            "reward for episode 2000: 5\n",
            "reward for episode 2100: -2\n",
            "reward for episode 2200: 8\n",
            "reward for episode 2300: 7\n",
            "reward for episode 2400: 9\n",
            "reward for episode 2500: 3\n",
            "reward for episode 2600: 6\n",
            "reward for episode 2700: 7\n",
            "reward for episode 2800: 8\n",
            "reward for episode 2900: 8\n",
            "reward for episode 3000: 10\n",
            "reward for episode 3100: 9\n",
            "reward for episode 3200: 13\n",
            "reward for episode 3300: 5\n",
            "reward for episode 3400: 10\n",
            "reward for episode 3500: 8\n",
            "reward for episode 3600: 5\n",
            "reward for episode 3700: 9\n",
            "reward for episode 3800: 12\n",
            "reward for episode 3900: 9\n",
            "reward for episode 4000: 10\n",
            "reward for episode 4100: 15\n",
            "reward for episode 4200: 4\n",
            "reward for episode 4300: 7\n",
            "reward for episode 4400: 10\n",
            "reward for episode 4500: 8\n",
            "reward for episode 4600: 8\n",
            "reward for episode 4700: 6\n",
            "reward for episode 4800: 10\n",
            "reward for episode 4900: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06ng5Utc8Gw",
        "outputId": "f3589447-1013-4642-d541-c4804bd50f59"
      },
      "source": [
        "windowed_rewards = np.convolve(rewards[-1000:], np.ones(100), 'valid')\n",
        "np.quantile(windowed_rewards/100,[0.05, 0.95])\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.41, 8.53])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xoqlbk5_pe"
      },
      "source": [
        "As shown above, the trained algorithm based on Q-learning achieves the 100-episode average reward with 5th percentile of 7.41 and 95th percentile of 8.53 over the last 1000 episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuzdVvrtv5Kw"
      },
      "source": [
        "# 5\n",
        "**The algorithm should be able to perform pick-ups and dropoffs with zero penalties over 1000 episodes.**\n",
        "\n",
        "As shown above, the program will assert when an illegal pick-up or dropoff (defined as those with -10 reward) occurs in the last 1000 episodes. \n",
        "The program did not assert showing that there is no illegal pick-up or dropoffs in the last 1000 epislodes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiTwqUEMwCUV"
      },
      "source": [
        "# 6\n",
        "**Document your solution including all hyper parameters and how those hyperparameters were selected.**\n",
        "\n",
        "I solved the taxi problem with the basic reinforcement learning Algorithm: Q-learning algorithm. The Q-learning is an off-policy temporal-difference learning algorithm.  \n",
        "\n",
        "At each step t, the action is chosen based on an epsilon-greedy approach\n",
        "a_t = argmax_a Q(s_t,a). The ε-greedy algorithm takes the best (greedy) action with probability 1-ε, and with probability ε it takes a random action. Typically, the initial ε is large to encourage more exploration, and as learning proceeds the ε is gradually reduced to favor the greedy action (exploitation). \n",
        "\n",
        "After applying action A_t, we observe reward R_{t+1} and get into the next state s_{t+1}.\n",
        "\n",
        "The action-value function is updated based on the following:\n",
        "\n",
        "Q(s_t,a_t) <-- Q(s_t,a_t)+ α*(R_{t+1} + γ * max_a Q(s_{t+1},a) - Q(s_t,a_t)).\n",
        "\n",
        "Note that unlike SARSA, at time t, Q-learning does not follow the current policy to pick the second action A_{t+1}, but instead just use the Q value associated with the greedy policy at t+1 for updating Q(s_t,a_t). \n",
        "\n",
        "The hyper parameters used in the taxi problem are as follows: \n",
        "* gamma = 0.9\n",
        "* n_episodes = 5000\n",
        "* max_episode_len = 100\n",
        "* epsilon = 1  \n",
        "* min_epsilon = 0.01\n",
        "* epsilon_decay = 0.99\n",
        "* alpha = 0.1\n",
        "\n",
        "I started with a much smaller alpha value (0.001), and because of the large state space it takes very long for the algorithm to update the Q value and hence learning the optimal policy. By increasing the alpha value to 0.1, the convergence is much faster.\n",
        "\n",
        "I also decreased the epsilon_decay rate from 0.999 to 0.99 to expedite the process of moving towards exploitation from exploration. This also helps with faster convergence.  \n",
        "\n",
        "The maximum episode length was chosen to be 100, as more than that is contributing very little to the cumulative return and a waste of learning time. \n",
        "\n",
        "Finally, it was found that with the above hyper parameters, the algorithm is able to achieve the desired performance in last 1000 episodes with a total 5000 training episodes. Using more training episodes is not necessary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63nRou-m5hv"
      },
      "source": [
        "## Evaluation\n",
        "The goal of the project is to get a certain average (cumulative) reward over 100 episodes. To pass the project, you must meet all the requirments in the project [rubric](https://github.com/KnowchowHQ/rl-in-action/blob/master/C1-RL-Intro/W3OH/P1-rubric.md)"
      ]
    }
  ]
}